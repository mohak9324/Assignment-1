{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a468a7-17dd-405a-a29a-1ea33183d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What are the different types of clustering algorithms, and how do they differ in terms of their approach\n",
    "# and underlying assumptions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d4f1df-8862-4f3e-8ffd-db28f7213eb0",
   "metadata": {},
   "source": [
    "Clustering algorithms are used to group a set of objects in such a way that objects in the same group (or cluster) are more similar to each other than to those in other groups. These algorithms differ in their approach, underlying assumptions, and the types of clusters they form. Here are several common types of clustering algorithms and how they differ:\n",
    "\n",
    "### 1. K-Means Clustering\n",
    "- **Approach**: Partitions the data into \\( K \\) pre-specified number of clusters. It assigns each data point to the closest cluster center and then updates the cluster center as the mean of the points in the cluster.\n",
    "- **Assumptions**: Assumes clusters are spherical and evenly sized, which means it works best when clusters are isotropic and of similar volume.\n",
    "- **Use Cases**: Works well for large datasets and is effective in finding spherical clusters.\n",
    "\n",
    "### 2. Hierarchical Clustering\n",
    "- **Approach**: Builds a tree of clusters by either a divisive method (splitting) or an agglomerative method (merging), creating a hierarchy of clusters.\n",
    "- **Assumptions**: Does not require the number of clusters to be specified in advance. Assumes that the data structure can be represented in a hierarchical manner.\n",
    "- **Use Cases**: Suitable for smaller datasets and can be used when the data shows a hierarchical relationship, allowing for a dendrogram to visualize the hierarchy.\n",
    "\n",
    "### 3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "- **Approach**: Forms clusters based on the density of data points, grouping together points that are closely packed together and marking points as outliers if they are in low-density regions.\n",
    "- **Assumptions**: Clusters are defined as areas of higher density than the remainder of the dataset, and can form arbitrary shapes.\n",
    "- **Use Cases**: Effective for datasets with clusters of varying shapes and sizes, and for identifying outliers.\n",
    "\n",
    "### 4. Mean Shift Clustering\n",
    "- **Approach**: Aims to discover blobs in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region.\n",
    "- **Assumptions**: There is no need to specify the number of clusters in advance, and it can find clusters of any shape.\n",
    "- **Use Cases**: Good for locating clusters in a dataset and can handle clusters of different shapes and sizes.\n",
    "\n",
    "### 5. Spectral Clustering\n",
    "- **Approach**: Uses eigenvalues of a similarity matrix to reduce dimensionality before applying another clustering method, typically K-Means, on the reduced space.\n",
    "- **Assumptions**: Assumes that the data points are connected in a graph and can be best separated through the graph's eigenvectors (spectral properties).\n",
    "- **Use Cases**: Effective for clustering data when the clusters are connected through a graph and are not necessarily compact or evenly distributed.\n",
    "\n",
    "### 6. Gaussian Mixture Models (GMM)\n",
    "- **Approach**: A probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.\n",
    "- **Assumptions**: Assumes data is generated from a mixture of several Gaussian distributions with unknown parameters.\n",
    "- **Use Cases**: Useful in scenarios where the clusters are assumed to be Gaussian distributed, allowing for more complex cluster shapes than K-Means.\n",
    "\n",
    "Each of these clustering algorithms has its strengths and weaknesses and is suitable for different types of data and clustering problems. The choice of algorithm often depends on the dataset characteristics, the desired clustering outcome, and the specific requirements of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22395747-4d25-4c95-9951-7df2cbc25aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.What is K-means clustering, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d3ce06-d862-4f7f-9720-9e4874949b82",
   "metadata": {},
   "source": [
    "K-means clustering is a popular partitioning method used in data analysis and machine learning to group a set of data points into K distinct, non-overlapping subgroups (or clusters) based on their features. The goal is to partition the data in such a way that points within the same cluster are more similar to each other than to points in other clusters. The similarity is typically measured using the Euclidean distance between points.\n",
    "\n",
    "### How K-means Clustering Works\n",
    "\n",
    "K-means clustering follows a simple and efficient iterative procedure to divide the data into K clusters:\n",
    "\n",
    "1. **Initialization**: Start by selecting \\( K \\) initial cluster centers (centroids). This can be done randomly, or more sophisticated methods can be used to choose the initial centroids.\n",
    "\n",
    "2. **Assignment Step**: Assign each data point to the closest centroid, where closeness is usually determined by the Euclidean distance between the data point and the centroid. After this step, each point is associated with one of the \\( K \\) clusters.\n",
    "\n",
    "3. **Update Step**: Update the centroid of each cluster to be the mean of the points assigned to that cluster. The mean calculation for each cluster involves summing all the vectors of the points in the cluster and dividing by the number of points.\n",
    "\n",
    "4. **Repeat**: Repeat the assignment and update steps until the centroids no longer change significantly or the assignment of points to clusters becomes stable. This means the algorithm has converged and found a local optimum.\n",
    "\n",
    "### Key Characteristics of K-means Clustering\n",
    "\n",
    "- **Number of Clusters \\( K \\)**: The number of clusters \\( K \\) needs to be specified in advance, and the quality of the final clusters can depend heavily on this choice.\n",
    "\n",
    "- **Sensitivity to Initial Centroids**: The initial selection of centroids can affect the final clustering outcome. Different initializations may lead to different results. To mitigate this issue, K-means is often run multiple times with different initializations, and the best clustering result (according to some criterion, like the total within-cluster variance) is chosen.\n",
    "\n",
    "- **Computational Efficiency**: K-means is generally efficient but can be computationally intensive on very large datasets, especially with a large number of features or clusters. However, it's more scalable compared to some other clustering methods, like hierarchical clustering.\n",
    "\n",
    "- **Assumptions and Limitations**: K-means assumes that clusters are spherical and of similar size, which may not always be true for real-world data. It also performs poorly on clusters of different sizes and densities, or non-spherical shapes.\n",
    "\n",
    "- **Use Cases**: K-means is widely used in market segmentation, document clustering, image segmentation, and anywhere else where data needs to be grouped into distinct categories based on their attributes.\n",
    "\n",
    "In summary, K-means is a versatile and straightforward clustering algorithm that is widely used across different domains for its simplicity and efficiency, although it has its limitations, particularly regarding the shape and size of clusters it can effectively identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839f9c20-6603-4704-8b14-380c9d446479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What are some advantages and limitations of K-means clustering compared to other clustering\n",
    "# techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f32f3-a890-43dd-8edb-05f20d1f8ac4",
   "metadata": {},
   "source": [
    "K-means clustering is one of the most popular clustering techniques due to its simplicity and efficiency. However, like all algorithms, it has its advantages and limitations, especially when compared to other clustering methods.\n",
    "\n",
    "### Advantages of K-means Clustering\n",
    "\n",
    "1. **Efficiency**: K-means is computationally efficient, especially for large datasets, making it suitable for a wide range of applications where quick, exploratory data analysis is needed.\n",
    "\n",
    "2. **Simplicity**: The algorithm is straightforward to implement and understand. Its process of assigning points to the nearest cluster center and then updating the centers is intuitive.\n",
    "\n",
    "3. **Scalability**: K-means can handle large datasets well, especially when optimized versions like Mini-Batch K-means are used.\n",
    "\n",
    "4. **Well-suited for Spherical Clusters**: K-means works very well when the clusters are distinct and roughly spherical or hyper-spherical in shape.\n",
    "\n",
    "5. **Feature Space**: It can be used with any type of feature space, as long as the concept of \"mean\" or \"center\" is meaningful, which makes it versatile across different domains.\n",
    "\n",
    "### Limitations of K-means Clustering\n",
    "\n",
    "1. **Number of Clusters**: K-means requires the number of clusters to be specified in advance, which is not always practical or intuitive, especially when the natural grouping in the data is unknown.\n",
    "\n",
    "2. **Sensitivity to Initial Centroids**: The results can be sensitive to the initial choice of centroids. Poor initialization can lead to suboptimal clustering. Methods like K-means++ have been developed to improve the initialization process.\n",
    "\n",
    "3. **Spherical Cluster Bias**: K-means assumes that clusters are spherical and of similar size. This can lead to poor performance on data with clusters of different sizes and shapes or clusters that are not spherical.\n",
    "\n",
    "4. **Handling of Outliers**: K-means is sensitive to noise and outliers, as a few such data points can significantly alter the mean of a cluster.\n",
    "\n",
    "5. **Local Minima**: K-means can converge to local minima, which means it may not find the best possible clustering solution. Running the algorithm multiple times with different initializations can help but does not guarantee the global optimum.\n",
    "\n",
    "### Comparison to Other Clustering Techniques\n",
    "\n",
    "- **Hierarchical Clustering**: Unlike K-means, hierarchical clustering does not require the number of clusters to be specified in advance and can provide a dendrogram that helps in understanding the data structure. However, it is computationally more intensive than K-means, especially for large datasets.\n",
    "\n",
    "- **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: DBSCAN does not require the number of clusters to be specified and can find clusters of arbitrary shapes, as well as identify outliers. In contrast to K-means, DBSCAN is more effective in handling data with varying densities and noise.\n",
    "\n",
    "- **Spectral Clustering**: This method is effective for identifying clusters that are not necessarily globular and can capture complex cluster structures better than K-means. However, spectral clustering can be computationally expensive for large datasets.\n",
    "\n",
    "- **Gaussian Mixture Models (GMM)**: GMM accommodates clusters of different sizes and shapes and provides soft clustering (i.e., providing probabilities of cluster memberships) unlike K-means, which assigns each point to a single cluster.\n",
    "\n",
    "In summary, while K-means is advantageous for its simplicity and efficiency, its limitations in handling different cluster shapes, sizes, and noise levels should be considered when choosing a clustering algorithm for a specific application. Other clustering techniques may be more suitable depending on the data characteristics and the analysis requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90903394-048f-4637-8365-4d93daf85adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. How do you determine the optimal number of clusters in K-means clustering, and what are some\n",
    "# common methods for doing so?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c1a2a8-a559-4251-ac92-5b466d417807",
   "metadata": {},
   "source": [
    "Determining the optimal number of clusters in K-means clustering is crucial for achieving meaningful and interpretable clustering results. Several methods can be used to estimate the best number of clusters, each with its own approach and criteria:\n",
    "\n",
    "### 1. Elbow Method\n",
    "- **Description**: The elbow method involves plotting the total within-cluster sum of squares (WSS) against the number of clusters and looking for the “elbow” point where the rate of decrease sharply changes. This point is often considered as an indicator of the optimal number of clusters.\n",
    "- **Usage**: Run K-means clustering for a range of cluster numbers (e.g., 1 to 10) and plot the WSS for each. The “elbow” is where adding another cluster doesn’t give much better modeling of the data.\n",
    "\n",
    "### 2. Silhouette Score\n",
    "- **Description**: The silhouette score measures how similar an object is to its own cluster compared to other clusters. The silhouette score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "- **Usage**: Compute the silhouette score for a range of number of clusters. The optimal number of clusters is the one that maximizes the average silhouette score.\n",
    "\n",
    "### 3. Gap Statistic\n",
    "- **Description**: The gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The optimal number of clusters is the value of k that maximizes the gap statistic.\n",
    "- **Usage**: The method involves generating a reference dataset, typically through Monte Carlo simulations, and calculating the gap statistic for different numbers of clusters. The optimal k is where the gap statistic is highest.\n",
    "\n",
    "### 4. Davies-Bouldin Index\n",
    "- **Description**: The Davies-Bouldin index is a metric for evaluating clustering algorithms. It is defined as the average similarity between each cluster and the cluster most similar to it, where similarity is a measure that compares the distance between clusters with the size of the clusters themselves.\n",
    "- **Usage**: The optimal number of clusters is the one that minimizes the Davies-Bouldin index.\n",
    "\n",
    "### 5. Cross-validation\n",
    "- **Description**: In cross-validation, the data is split into subsets, and the clustering algorithm is run separately on each subset. The idea is to find the number of clusters that provides the best stability across the subsets.\n",
    "- **Usage**: The number of clusters is chosen based on how well the clustering results can be replicated on different subsets of the data.\n",
    "\n",
    "### Considerations\n",
    "- No single method is universally best, and different methods may suggest different numbers of clusters for the same dataset. It’s often useful to consider several methods and use domain knowledge or additional criteria to make a final decision.\n",
    "- The chosen method should align with the specific goals and constraints of the clustering analysis, as well as the nature of the dataset being analyzed.\n",
    "\n",
    "In practice, determining the optimal number of clusters often involves a combination of these methods, along with trial and error, and domain expertise to interpret the results meaningfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9517cbb-5207-4807-950c-257d71e7b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What are some applications of K-means clustering in real-world scenarios, and how has it been used\n",
    "# to solve specific problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d07bbc-3edd-4778-be80-646a814f4cf8",
   "metadata": {},
   "source": [
    "K-means clustering is widely used in various fields due to its simplicity, efficiency, and effectiveness in grouping data into distinct categories based on their attributes. Here are some real-world applications of K-means clustering:\n",
    "\n",
    "### 1. Market Segmentation\n",
    "- **Application**: Businesses use K-means to segment customers based on features like buying behavior, demographics, and psychographics to tailor marketing strategies, optimize product placements, and personalize customer experiences.\n",
    "- **Example**: A retail company might use K-means to identify customer segments that prefer certain product categories, enabling targeted marketing campaigns and personalized promotions.\n",
    "\n",
    "### 2. Document Clustering\n",
    "- **Application**: K-means is used in text mining to group similar documents together, facilitating information retrieval, organizing content, and understanding document themes.\n",
    "- **Example**: News agencies can use K-means to categorize news articles into topics like sports, politics, entertainment, etc., helping in automated news aggregation and faster information retrieval.\n",
    "\n",
    "### 3. Image Segmentation\n",
    "- **Application**: In image processing, K-means clustering is used for segmenting images into regions of similar colors or intensities, which is useful in object recognition, image compression, and analysis.\n",
    "- **Example**: In medical imaging, K-means can help segment different tissue types in MRI scans, aiding in the diagnostic process by highlighting areas of interest.\n",
    "\n",
    "### 4. Anomaly Detection\n",
    "- **Application**: K-means can be used to detect anomalies or outliers in datasets by identifying small clusters or points far from the centroid of their nearest cluster.\n",
    "- **Example**: In network security, K-means clustering can help identify unusual patterns or activities that could indicate a cybersecurity threat or breach.\n",
    "\n",
    "### 5. Inventory Categorization\n",
    "- **Application**: K-means clustering helps in inventory management by categorizing products into groups based on sales activity, demand patterns, and other characteristics.\n",
    "- **Example**: A manufacturing company could use K-means to optimize its inventory levels by grouping products based on sales velocity, leading to more efficient stock replenishment and storage.\n",
    "\n",
    "### 6. Feature Engineering\n",
    "- **Application**: In machine learning, K-means can be used for feature engineering by creating new features that represent the membership of data points to clusters, which can improve the performance of predictive models.\n",
    "- **Example**: In real estate pricing models, K-means clustering can be used to create neighborhood clusters that can then be used as features in predicting property prices.\n",
    "\n",
    "### 7. Social Network Analysis\n",
    "- **Application**: K-means is applied in social network analysis to detect communities or groups within larger networks, based on the interactions and connections between members.\n",
    "- **Example**: Social media platforms might use K-means to identify user groups with similar interests to recommend content or advertisements effectively.\n",
    "\n",
    "These applications demonstrate the versatility of K-means clustering in extracting meaningful patterns and groups from data across various domains. The algorithm's ability to handle large datasets and provide actionable insights makes it a valuable tool in data analysis and decision-making processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "691b8a09-263d-405e-a72a-7ba00bbb8e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. How do you interpret the output of a K-means clustering algorithm, and what insights can you derive\n",
    "# from the resulting clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb1700-6d50-46ce-9e62-4e85c19a518a",
   "metadata": {},
   "source": [
    "Interpreting the output of a K-means clustering algorithm involves analyzing the characteristics of the data points within each cluster and understanding how they differ from those in other clusters. Here's how you can interpret the results and derive insights:\n",
    "\n",
    "### 1. Cluster Characteristics\n",
    "- **Centroids**: Examine the centroids of the clusters, which represent the mean of the features for the points in each cluster. The centroids give an overview of the defining characteristics of each cluster.\n",
    "- **Feature Values**: Look at the average values of the features within each cluster. High or low values in certain features can help you understand what distinguishes each cluster from the others.\n",
    "\n",
    "### 2. Cluster Size\n",
    "- **Distribution of Data Points**: Notice how many data points are in each cluster. A very large or very small cluster could indicate common patterns or outliers, respectively, and might require further investigation.\n",
    "\n",
    "### 3. Data Distribution within Clusters\n",
    "- **Spread and Variability**: Analyze the spread and variability within each cluster. A tight, compact cluster may indicate a strong, well-defined grouping, while a dispersed cluster might suggest a more heterogeneous grouping.\n",
    "\n",
    "### 4. Comparing Clusters\n",
    "- **Differences and Similarities**: Compare clusters to each other to identify key differences and similarities. This can reveal relationships between different groups in your data and help identify unique or shared characteristics.\n",
    "\n",
    "### Insights You Can Derive\n",
    "\n",
    "- **Identifying Subgroups**: Clusters can reveal distinct subgroups within your data, helping to identify patterns or behaviors that are not immediately obvious. For example, in customer data, you might find segments based on purchasing behavior or preferences.\n",
    "- **Informing Decision Making**: Insights from clustering can inform strategic decisions, such as targeting specific customer segments with tailored marketing campaigns or adjusting product features to better suit different user groups.\n",
    "- **Trend Analysis**: Over time, clustering can help in tracking changes and trends in various segments. For example, in sales data, tracking how clusters of products change over time can provide insights into changing consumer preferences.\n",
    "- **Resource Allocation**: In operations or resource management, clustering can help optimize resource allocation by identifying groups with similar needs or characteristics.\n",
    "- **Anomaly Detection**: Clusters with very few data points might indicate anomalies or outliers, which could be significant in fraud detection, quality control, or error identification.\n",
    "\n",
    "### Practical Steps\n",
    "\n",
    "- **Labeling Clusters**: Assign meaningful labels to each cluster based on their defining characteristics. This makes the results more interpretable and actionable.\n",
    "- **Visual Analysis**: Use visual tools like scatter plots, bar charts, or heatmaps to compare the clusters and understand their characteristics.\n",
    "- **Cross-Validation**: Validate the clustering results with domain experts or through external data to ensure that the clusters make practical sense and align with known patterns or theories.\n",
    "\n",
    "By carefully interpreting the clusters formed by K-means and understanding the characteristics of each group, you can extract valuable insights that drive informed decisions and strategies in various contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac68c466-828b-457b-b8f2-1377ce62b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. What are some common challenges in implementing K-means clustering, and how can you address\n",
    "# them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592eaff-04ab-4733-8686-c20e1e55dfc8",
   "metadata": {},
   "source": [
    "Implementing K-means clustering can present several challenges that can affect the quality of the clustering results. Here are some common challenges and strategies to address them:\n",
    "\n",
    "### 1. Choosing the Number of Clusters\n",
    "- **Challenge**: Determining the optimal number of clusters (\\( K \\)) is not straightforward and can significantly impact the outcome.\n",
    "- **Solution**: Use methods like the Elbow method, Silhouette analysis, Gap statistic, or Davies-Bouldin index to evaluate the optimal number of clusters. Experiment with different values and analyze the resulting cluster quality to make an informed decision.\n",
    "\n",
    "### 2. Sensitivity to Initial Centroids\n",
    "- **Challenge**: K-means is sensitive to the initial choice of centroids, which can lead to different clustering results on different runs.\n",
    "- **Solution**: Use the K-means++ algorithm for smarter initialization of centroids, reducing the likelihood of poor cluster formation. Alternatively, run K-means multiple times with different random seeds and choose the best outcome based on a consistency criterion or the lowest within-cluster sum of squares.\n",
    "\n",
    "### 3. Handling Non-Spherical Clusters\n",
    "- **Challenge**: K-means assumes that clusters are spherical and of similar size, which may not hold true for all datasets, leading to poor clustering performance for elongated or irregularly shaped clusters.\n",
    "- **Solution**: Preprocess the data with PCA or another dimensionality reduction technique to remove noise and reveal structure. Consider alternative clustering algorithms like Spectral clustering or DBSCAN for data with complex shapes.\n",
    "\n",
    "### 4. Scalability with Large Datasets\n",
    "- **Challenge**: K-means can be computationally expensive, especially with very large datasets and a high number of features.\n",
    "- **Solution**: Use Mini-Batch K-means, which processes the data in small batches and is faster and more scalable. Another approach is to reduce the dimensionality of the data before clustering.\n",
    "\n",
    "### 5. Clusters of Varying Density and Size\n",
    "- **Challenge**: K-means may struggle with clusters of varying density and size, often biasing towards equal-sized clusters.\n",
    "- **Solution**: Consider using a density-based clustering algorithm like DBSCAN, which can handle clusters of varying density and size better. Alternatively, normalize or scale the data appropriately to ensure that all dimensions contribute equally to the distance computations.\n",
    "\n",
    "### 6. Outliers and Noise\n",
    "- **Challenge**: Outliers and noise can heavily influence the mean calculation in K-means, potentially skewing clusters.\n",
    "- **Solution**: Preprocess the data to remove or reduce the impact of outliers, for example, using outlier detection techniques or robust scaling methods. Regularly review and clean the data to ensure quality.\n",
    "\n",
    "### 7. Evaluating Cluster Quality\n",
    "- **Challenge**: Assessing the quality and validity of the clusters produced can be difficult, especially without ground truth labels.\n",
    "- **Solution**: Use internal validation metrics like silhouette score, Davies-Bouldin index, or cohesion and separation measures to evaluate the goodness of the clustering. External validation can be performed if labeled data is available, using metrics like purity, normalized mutual information, or Rand index.\n",
    "\n",
    "Addressing these challenges requires a combination of good preprocessing practices, careful method selection, and thorough validation to ensure that the K-means clustering results are robust, meaningful, and useful for the intended application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7d06f-eeb6-4de4-a98c-ac246cefb5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
