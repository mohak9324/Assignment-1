{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c6ac16f-9be0-43e1-8bce-4886b21d2039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "# company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "# probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec0bcbc-bef3-43b2-93b0-1b7e0e78fe94",
   "metadata": {},
   "source": [
    "70% of the employees use the company's health insurance plan.\n",
    "40% of the employees who use the plan are smokers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c12dc-9326-4302-a9f6-7139e0d6ce09",
   "metadata": {},
   "source": [
    "P(S): probability that an employee is a smoker.\n",
    "P(I): probability that an employee uses the health insurance plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3549eee0-755c-460f-810c-aff04cc15203",
   "metadata": {},
   "source": [
    "P(I)=0.7\n",
    "P(S|I)=0.4   probability that an employee is a smoker given that they use the health insurance plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f5ef8-6f61-4f62-808d-db34fef10d67",
   "metadata": {},
   "source": [
    "P(S|I) will be 0.4 i.e 40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f6d22bf-efff-4e01-a5b9-6c5ed06ae895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa898783-65ba-444e-a4c0-65e86e00d9d8",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm, each designed to handle different types of data inputs. Here's a comparison of the two:\n",
    "\n",
    "### Bernoulli Naive Bayes:\n",
    "\n",
    "1. **Data Type**: It is specifically used for binary/boolean features. That is, it's suitable for data where features are either present or absent (typically represented as 1s and 0s).\n",
    "\n",
    "2. **Modeling Technique**: In Bernoulli Naive Bayes, the feature vectors represent the presence or absence of features. For example, in text classification, it might represent whether a specific word appears in a document or not, regardless of the number of times it appears.\n",
    "\n",
    "3. **Use Cases**: It's often used in text classification where the \"bag of words\" model is binary. For instance, classifying emails as spam or not spam based on the presence or absence of certain keywords.\n",
    "\n",
    "4. **Probability Estimation**: The probability of a feature being present or absent in a particular class is estimated and used for prediction.\n",
    "\n",
    "### Multinomial Naive Bayes:\n",
    "\n",
    "1. **Data Type**: It is designed for features that represent counts or frequency counts. The features are typically the counts of events or occurrences.\n",
    "\n",
    "2. **Modeling Technique**: In Multinomial Naive Bayes, the algorithm counts the occurrence of each feature (e.g., word in text classification) and uses these counts to make predictions.\n",
    "\n",
    "3. **Use Cases**: Commonly used in text classification (e.g., categorizing documents into different topics) where the frequency of words is important. It works well with term frequency or TF-IDF weighted word counts.\n",
    "\n",
    "4. **Probability Estimation**: The probability of observing a certain count of a feature in a particular class is estimated.\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "- **Feature Representation**: Bernoulli Naive Bayes is binary and focuses on the presence/absence of features, while Multinomial Naive Bayes deals with the frequency/count of features.\n",
    "\n",
    "- **Applicability**: Bernoulli is more suited for binary feature models, while Multinomial is better for features that can take on more than two values (like word counts).\n",
    "\n",
    "- **Use Case Scenarios**: While both are used in text classification, their applicability depends on how the text data is vectorized. Bernoulli is used when only the appearance of words matters, whereas Multinomial is used when the frequency of words is important. \n",
    "\n",
    "Choosing between these two depends on the nature of your data and the specific requirements of your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a61d649-e5d7-4522-b136-a49a2b246ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddce854-5de7-45b9-aca5-1624a6669e49",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes, like other Naive Bayes variants, is fundamentally a probabilistic model that calculates the likelihood of different classes based on the presence or absence of features. How it handles missing values in the data depends largely on the implementation and data preprocessing steps. However, there are some general approaches:\n",
    "\n",
    "1. **Ignoring Missing Values During Model Training and Prediction**:\n",
    "   - When computing probabilities, Bernoulli Naive Bayes typically considers only the presence or absence of features. If a feature is missing (i.e., not present), it can be treated as absent. This approach essentially ignores missing values during probability calculation.\n",
    "\n",
    "2. **Data Imputation**:\n",
    "   - Before feeding data into the model, missing values can be imputed during the preprocessing stage. Common imputation strategies include:\n",
    "     - Replacing missing values with the most frequent value in a column.\n",
    "     - Using a placeholder value that represents \"unknown\" or \"missing\".\n",
    "     - Applying more sophisticated imputation methods based on other features in the dataset.\n",
    "   - However, imputation should be done carefully, considering the nature of the data and the implications of the chosen imputation method on the model's performance.\n",
    "\n",
    "3. **Custom Handling in Implementation**:\n",
    "   - Custom modifications to the Bernoulli Naive Bayes algorithm can be made to handle missing values in a specific way, depending on the requirements of the task. For example, one might modify the likelihood calculations to account for missingness as a separate category.\n",
    "\n",
    "4. **Modeling Missing Values as a Separate Category**:\n",
    "   - In some implementations, missing values can be explicitly modeled as a separate category. This approach treats the missingness of the data as informative by itself.\n",
    "\n",
    "### Important Considerations:\n",
    "\n",
    "- **Impact on Model Performance**: How missing values are handled can significantly impact the model's performance. Careful validation and testing are necessary to understand the impact.\n",
    "\n",
    "- **Data Nature and Context**: The approach for handling missing values should align with the nature of the data and the specific problem context.\n",
    "\n",
    "In practice, Bernoulli Naive Bayes, due to its simplicity and the way it handles features (as binary variables), can be quite robust to missing data, especially when missingness aligns with the binary absence of a feature. However, for more complex datasets or when missing values carry important information, more sophisticated handling may be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac1a82c-229c-4048-8cae-6de4106c3b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f6673-26be-4395-802b-ce5e1ddbed0c",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification problems. The Gaussian Naive Bayes classifier is particularly well-suited for continuous input features and is based on the assumption that the continuous values associated with each class are distributed according to a Gaussian (normal) distribution.\n",
    "\n",
    "### Multi-Class Classification with Gaussian Naive Bayes:\n",
    "\n",
    "1. **Basic Principle**:\n",
    "   - In a multi-class setting, the Gaussian Naive Bayes model calculates the probability of each class given an input feature vector and then predicts the class with the highest probability.\n",
    "   - For each class, it assumes that the features follow a Gaussian distribution, and it calculates the mean and variance of the features in each class during the training process.\n",
    "\n",
    "2. **Probability Calculation**:\n",
    "   - For a given input feature vector, the model calculates the probability of that vector belonging to each class, based on the Gaussian distribution of the features in that class.\n",
    "   - It applies Bayes' theorem to compute the posterior probability for each class, and then it selects the class with the highest posterior probability.\n",
    "\n",
    "3. **Applicability**:\n",
    "   - Gaussian Naive Bayes is used in a variety of multi-class classification scenarios, especially where features are continuous. Common applications include text classification, medical diagnosis, and more.\n",
    "\n",
    "4. **Advantages**:\n",
    "   - Gaussian Naive Bayes is easy to implement and fast, making it suitable for large datasets.\n",
    "   - It performs well in multi-class classification even with the assumption of feature independence, which is the \"naive\" part of Naive Bayes.\n",
    "\n",
    "5. **Limitations**:\n",
    "   - The assumption that features are normally distributed might not always hold true, which can affect the model's performance.\n",
    "   - The independence assumption between features is often violated in real-world data, but in practice, Naive Bayes classifiers can still perform well even when this independence assumption is not strictly true.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Gaussian Naive Bayes is a versatile algorithm that can efficiently handle multi-class classification problems, especially when the input features are continuous and can be reasonably approximated by a Gaussian distribution. As with any model, its effectiveness can depend on the specific characteristics of the dataset and the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f8f28c-be61-4577-b97b-0497da8124fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation:\n",
    "# Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "# datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "# is spam or not based on several input features.\n",
    "\n",
    "# Implementation:\n",
    "# # Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "# # scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "# # dataset. You should use the default hyperparameters for each classifier.\n",
    "# # Results:\n",
    "# # Report the following performance metrics for each classifier:\n",
    "# # Accuracy\n",
    "# # Precision\n",
    "# # Recall\n",
    "# # F1 score\n",
    "# # Discussion:\n",
    "# # Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "# # the case? Are there any limitations of Naive Bayes that you observed?\n",
    "# # Conclusion:\n",
    "# # Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afacf7cc-ac13-4095-8ef0-a31d948f9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold,cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d469e055-d561-4ec7-82cd-d8944320a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('spambase.data',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e9d860d-8ff8-4803-a695-0715976622fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['word_freq_make',\n",
    "'word_freq_address',\n",
    "'word_freq_all',\n",
    "'word_freq_3d',\n",
    "'word_freq_our',\n",
    "'word_freq_over',\n",
    "'word_freq_remove',\n",
    "'word_freq_internet',\n",
    "'word_freq_order',\n",
    "'word_freq_mail',\n",
    "'word_freq_receive',\n",
    "'word_freq_will',\n",
    "'word_freq_people',\n",
    "'word_freq_report',\n",
    "'word_freq_addresses',\n",
    "'word_freq_free',\n",
    "'word_freq_business',\n",
    "'word_freq_email',\n",
    "'word_freq_you',\n",
    "'word_freq_credit',\n",
    "'word_freq_your',\n",
    "'word_freq_font',\n",
    "'word_freq_000',\n",
    "'word_freq_money',\n",
    "'word_freq_hp',\n",
    "'word_freq_hpl',\n",
    "'word_freq_george',\n",
    "'word_freq_650',\n",
    "'word_freq_lab',\n",
    "'word_freq_labs',\n",
    "'word_freq_telnet',\n",
    "'word_freq_857',\n",
    "'word_freq_data',\n",
    "'word_freq_415',\n",
    "'word_freq_85',\n",
    "'word_freq_technology',\n",
    "'word_freq_1999',\n",
    "'word_freq_parts',\n",
    "'word_freq_pm',\n",
    "'word_freq_direct',\n",
    "'word_freq_cs',\n",
    "'word_freq_meeting',\n",
    "'word_freq_original',\n",
    "'word_freq_project',\n",
    "'word_freq_re',\n",
    "'word_freq_edu',\n",
    "'word_freq_table',\n",
    "'word_freq_conference',\n",
    "'char_freq_;',\n",
    "'char_freq_(',\n",
    "'char_freq_[',\n",
    "'char_freq_!',\n",
    "'char_freq_$',\n",
    "'char_freq_#',\n",
    "'capital_run_length_average',\n",
    "'capital_run_length_longest',\n",
    "'capital_run_length_total',\n",
    "'spam'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d97cbb5-feca-4730-a9f5-e98fa637764c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                       278     1  \n",
       "1                      1028     1  \n",
       "2                      2259     1  \n",
       "3                       191     1  \n",
       "4                       191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10e48325-e21f-4c39-85c8-773d074a4ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_freq_make                0\n",
       "word_freq_address             0\n",
       "word_freq_all                 0\n",
       "word_freq_3d                  0\n",
       "word_freq_our                 0\n",
       "word_freq_over                0\n",
       "word_freq_remove              0\n",
       "word_freq_internet            0\n",
       "word_freq_order               0\n",
       "word_freq_mail                0\n",
       "word_freq_receive             0\n",
       "word_freq_will                0\n",
       "word_freq_people              0\n",
       "word_freq_report              0\n",
       "word_freq_addresses           0\n",
       "word_freq_free                0\n",
       "word_freq_business            0\n",
       "word_freq_email               0\n",
       "word_freq_you                 0\n",
       "word_freq_credit              0\n",
       "word_freq_your                0\n",
       "word_freq_font                0\n",
       "word_freq_000                 0\n",
       "word_freq_money               0\n",
       "word_freq_hp                  0\n",
       "word_freq_hpl                 0\n",
       "word_freq_george              0\n",
       "word_freq_650                 0\n",
       "word_freq_lab                 0\n",
       "word_freq_labs                0\n",
       "word_freq_telnet              0\n",
       "word_freq_857                 0\n",
       "word_freq_data                0\n",
       "word_freq_415                 0\n",
       "word_freq_85                  0\n",
       "word_freq_technology          0\n",
       "word_freq_1999                0\n",
       "word_freq_parts               0\n",
       "word_freq_pm                  0\n",
       "word_freq_direct              0\n",
       "word_freq_cs                  0\n",
       "word_freq_meeting             0\n",
       "word_freq_original            0\n",
       "word_freq_project             0\n",
       "word_freq_re                  0\n",
       "word_freq_edu                 0\n",
       "word_freq_table               0\n",
       "word_freq_conference          0\n",
       "char_freq_;                   0\n",
       "char_freq_(                   0\n",
       "char_freq_[                   0\n",
       "char_freq_!                   0\n",
       "char_freq_$                   0\n",
       "char_freq_#                   0\n",
       "capital_run_length_average    0\n",
       "capital_run_length_longest    0\n",
       "capital_run_length_total      0\n",
       "spam                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31ddf40a-7113-490d-bf21-f91f264f978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('spam',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a61fefa1-1f80-49e2-a23e-507bcfb03dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b70a4119-31c5-4ca6-a082-efee1f3c0c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier={'BernoulliNB':BernoulliNB(),\n",
    "           'MultinomialNB':MultinomialNB(),\n",
    "           'GaussianNB':GaussianNB()}\n",
    "\n",
    "cv=StratifiedKFold(n_splits=10)\n",
    "\n",
    "scoring_metrics = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1_score': make_scorer(f1_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4a46ded-5ff1-4132-ba7b-f0b4c896e6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BernoulliNB': {'accuracy': 0.8839380364047911,\n",
       "  'precision': 0.8869617393737383,\n",
       "  'recall': 0.8152389047416673,\n",
       "  'f1_score': 0.8481249015095276},\n",
       " 'MultinomialNB': {'accuracy': 0.7863496180326323,\n",
       "  'precision': 0.7393175533565436,\n",
       "  'recall': 0.7214983911116508,\n",
       "  'f1_score': 0.7282909724016348},\n",
       " 'GaussianNB': {'accuracy': 0.8217730830896915,\n",
       "  'precision': 0.7103733928118492,\n",
       "  'recall': 0.9569516119239877,\n",
       "  'f1_score': 0.8130660909542995}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "for name, clf in classifier.items():\n",
    "    #print(clf)\n",
    "    clf_results = {}\n",
    "    for metric, scorer in scoring_metrics.items():\n",
    "        scores = cross_val_score(clf, X, y, scoring=scorer, cv=cv)\n",
    "        clf_results[metric] = scores.mean()\n",
    "    results[name] = clf_results\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392a43f-b471-409d-b119-86e29cd43939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
