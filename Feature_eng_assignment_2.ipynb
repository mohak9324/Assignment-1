{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed85e0e-33ed-400b-a5d8-3463e73c77fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "# application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b10b7f1-6de1-4081-b1f4-7012d0712711",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as Min-Max normalization, is a data preprocessing technique used to scale and transform features in a dataset to a specific range, typically between 0 and 1. The purpose of Min-Max scaling is to ensure that all features have the same scale, preventing some features from dominating others in machine learning algorithms that rely on feature magnitudes, such as gradient descent.\n",
    "\n",
    "X_scaled= X- X_min / X_max - X_min\n",
    "\n",
    "X_Min= is the minimum value of feature X in the dataset.\n",
    "X_max=is the Maximum value of feature X in the dataset.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset of exam scores, where the scores range from 60 to 100. The goal is to scale these scores to a range between 0 and 1 using Min-Max scaling.\n",
    "\n",
    "Let's scale a score of 80 using Min-Max scaling:\n",
    "\n",
    "=80-100/100-60\n",
    "0.5\n",
    "\n",
    "Benefits and Considerations of Min-Max Scaling:\n",
    "\n",
    "Min-Max scaling transforms all features to the same scale, which is important for many machine learning algorithms, especially those based on distances or gradients.\n",
    "It preserves the relative relationships between values in each feature, which can be useful when the actual values are important.\n",
    "However, Min-Max scaling is sensitive to outliers, and extreme values in the dataset can significantly affect the scaling, potentially compressing most values into a narrow range. In such cases, robust scaling techniques like Z-score scaling (standardization) may be preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b8c05c-3d32-4250-9a28-5fa58efb9327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "# Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9466573d-4c47-4204-8d12-3b11f9ba39e2",
   "metadata": {},
   "source": [
    "The **Unit Vector** technique in feature scaling, also known as vector normalization or the L2 normalization, is a method used to scale features in a dataset such that each feature vector (observation) has a Euclidean length (L2 norm) of 1. This technique transforms the data in such a way that it lies on the surface of a unit hypersphere.\n",
    "\n",
    "The formula for Unit Vector scaling of a feature vector \\(X\\) is as follows:\n",
    "\n",
    "X/||X||\n",
    "\n",
    "Where:\n",
    "X = original vector\n",
    "||X|| = unit vector of X\n",
    "\n",
    "\n",
    "Unit Vector scaling ensures that each feature vector has a constant magnitude (length) of 1 while preserving the direction of the original vector. This scaling technique is particularly useful in scenarios where the direction of the feature vectors is more important than their magnitudes.\n",
    "\n",
    "\n",
    "**Difference from Min-Max Scaling:**\n",
    "\n",
    "The key difference between Unit Vector scaling and Min-Max scaling is the scaling approach:\n",
    "\n",
    "- **Unit Vector Scaling:** Unit Vector scaling normalizes each feature vector to have a constant length of 1 while preserving direction.\n",
    "  \n",
    "- **Min-Max Scaling:** Min-Max scaling scales features to a specific range (typically between 0 and 1) while preserving the relative relationships between values in each feature.\n",
    "\n",
    "Unit Vector scaling does not change the magnitude of the original feature values but rather their proportion to each other. Min-Max scaling, on the other hand, transforms feature values to a predefined range, which can be any specified interval. The choice between these scaling techniques depends on the specific requirements of the machine learning problem and the nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7d5ec3-1300-4cc1-a55f-79cb8a283b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3 What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "# example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d8aa7-9bcd-4b4a-9669-646811168b2e",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique commonly used in data analysis and machine learning. It is primarily used to reduce the number of features (dimensions) in a dataset while preserving as much of the original variance as possible. PCA achieves this by transforming the data into a new coordinate system defined by its principal components, which are linear combinations of the original features. The principal components are ordered in terms of their ability to explain the variance in the data, with the first principal component capturing the most variance.\n",
    "\n",
    "Here's how PCA works:\n",
    "\n",
    "1. **Centering the Data:** PCA starts by centering the data, which means subtracting the mean of each feature from the data points. This ensures that the transformed data has a mean of zero.\n",
    "\n",
    "2. **Calculating Covariance Matrix:** PCA calculates the covariance matrix of the centered data. The covariance matrix describes the relationships between pairs of features.\n",
    "\n",
    "3. **Eigenvalue Decomposition:** PCA performs an eigenvalue decomposition (or singular value decomposition) of the covariance matrix to obtain the eigenvectors and eigenvalues. The eigenvectors are the principal components, and the eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "4. **Selecting Principal Components:** The principal components are sorted in descending order of their corresponding eigenvalues. Typically, you select the top \\(k\\) principal components that explain most of the variance, where \\(k\\) is the desired reduced dimensionality.\n",
    "\n",
    "5. **Projecting Data:** Finally, PCA projects the data onto the selected principal components to obtain a reduced-dimensional representation of the data.\n",
    "\n",
    "Here's an example to illustrate the application of PCA:\n",
    "\n",
    "Suppose you have a dataset with two features, \"Height\" and \"Weight,\" and you want to reduce the dimensionality of the data using PCA. The goal is to represent the data in one dimension (from 2D to 1D) while preserving as much variance as possible.\n",
    "\n",
    "- **Original Data (2D):**\n",
    "  - Sample 1: [170 cm, 68 kg]\n",
    "  - Sample 2: [160 cm, 55 kg]\n",
    "  - Sample 3: [175 cm, 75 kg]\n",
    "  - ...\n",
    "\n",
    "After centering the data, calculating the covariance matrix, and performing eigenvalue decomposition, you find that the first principal component (PC1) explains 95% of the variance, and the second principal component (PC2) explains only 5% of the variance.\n",
    "\n",
    "You decide to reduce the data to one dimension by selecting only PC1. Now, you project the original data onto PC1:\n",
    "\n",
    "- **Reduced Data (1D):**\n",
    "  - Sample 1: [5.31] (projected value)\n",
    "  - Sample 2: [-1.12] (projected value)\n",
    "  - Sample 3: [8.97] (projected value)\n",
    "  - ...\n",
    "\n",
    "The reduced data has only one feature, which is a linear combination of the original \"Height\" and \"Weight\" features. While dimensionality is reduced, much of the original variance is still captured in this reduced representation.\n",
    "\n",
    "PCA is widely used in various applications, including image compression, data visualization, and noise reduction, as it allows you to reduce the dimensionality of complex datasets while preserving essential information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15a33a5d-fe9f-4488-acb5-b3fb07ff96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "# Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c5a546-ef3c-4060-b18c-a7e9c3aa9e82",
   "metadata": {},
   "source": [
    "**PCA (Principal Component Analysis)** and **feature extraction** are related concepts in the context of dimensionality reduction, but they serve different purposes and have distinct methodologies.\n",
    "\n",
    "**PCA** is primarily a dimensionality reduction technique that transforms the original features into a new set of orthogonal features (principal components) while preserving as much of the original variance as possible. The principal components are linear combinations of the original features and are ranked by their ability to explain the variance in the data.\n",
    "\n",
    "**Feature extraction**, on the other hand, is a broader concept that involves creating new features from the original ones to capture specific information or patterns in the data. It's not limited to dimensionality reduction but can also be used for enhancing certain aspects of the data representation, feature engineering, or creating informative representations for machine learning tasks.\n",
    "\n",
    "Now, let's discuss how PCA can be used for feature extraction:\n",
    "\n",
    "**Feature Extraction using PCA:**\n",
    "\n",
    "PCA can be employed as a feature extraction technique when you are interested in creating a more compact and informative feature set while maintaining as much useful information as possible. Here's how it works:\n",
    "\n",
    "1. **Data Preprocessing:** Start with your original dataset containing \\(n\\) samples and \\(m\\) original features.\n",
    "\n",
    "2. **Center the Data:** Subtract the mean of each feature to center the data.\n",
    "\n",
    "3. **Perform PCA:** Apply PCA to the centered data to obtain the principal components (PCs).\n",
    "\n",
    "4. **Select Components:** Choose a subset of the top \\(k\\) principal components (where \\(k < m\\)) based on their corresponding eigenvalues or the amount of variance they explain. These selected PCs will serve as the new features.\n",
    "\n",
    "5. **Create the Transformed Dataset:** Project the centered data onto the selected PCs to create a new dataset with \\(k\\) features, which are linear combinations of the original features.\n",
    "\n",
    "6. **Use the Transformed Features:** The transformed dataset with \\(k\\) features can be used for further analysis, machine learning, or other tasks.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's consider an example with a dataset of images, where each image is represented by a high-dimensional vector of pixel values. You want to extract informative features from these images for a classification task while reducing the dimensionality.\n",
    "\n",
    "1. **Original Data:** Images represented by pixel values (e.g., 1000-dimensional vectors).\n",
    "\n",
    "2. **Center the Data:** Subtract the mean pixel vector from each image to center the data.\n",
    "\n",
    "3. **PCA:** Apply PCA to the centered data to obtain principal components.\n",
    "\n",
    "4. **Select Components:** Choose the top \\(k\\) principal components (e.g., 50) based on their explained variance.\n",
    "\n",
    "5. **Create Transformed Dataset:** Project each centered image onto the selected principal components to create a new dataset with \\(k\\) features.\n",
    "\n",
    "6. **Use Transformed Features:** Train a machine learning model on the reduced-dimensional dataset with \\(k\\) features for image classification.\n",
    "\n",
    "In this example, PCA was used for feature extraction to create a more compact and informative representation of the images while reducing dimensionality. The extracted features can then be used as inputs to a machine learning model for classification or other tasks.\n",
    "\n",
    "Overall, PCA can be a valuable tool for feature extraction when you want to reduce dimensionality while preserving important information in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "738b6fe9-d3b2-4678-a616-76c798a90a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "# contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "# preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36bc6177-7640-40cd-9266-910a07baa9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Price    Rating  DeliveryTime\n",
      "0   0.00  0.333333      0.000000\n",
      "1   0.50  0.666667      0.500000\n",
      "2   1.00  0.000000      1.000000\n",
      "3   0.25  1.000000      0.333333\n",
      "4   0.75  0.111111      0.833333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample dataset (replace this with your actual dataset)\n",
    "data = {\n",
    "    'Price': [10, 20, 30, 15, 25],\n",
    "    'Rating': [4.2, 4.5, 3.9, 4.8, 4.0],\n",
    "    'DeliveryTime': [30, 45, 60, 40, 55]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the Min-Max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Specify the columns you want to scale (in this case, all columns)\n",
    "columns_to_scale = df.columns\n",
    "\n",
    "# Apply Min-Max scaling to the selected columns\n",
    "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "# Display the scaled DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e611ba-455c-448b-bd8a-5166e8d98bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "# features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "# dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78d010-e8e5-43a9-a2c7-3b1d481e64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load your stock price dataset (replace with your dataset)\n",
    "# Assume your dataset has features like company financial data and market trends.\n",
    "# For demonstration, we'll generate a sample dataset.\n",
    "data = {\n",
    "    'Feature1': [100, 200, 300, 400],\n",
    "    'Feature2': [50, 75, 125, 150],\n",
    "    'Feature3': [5, 10, 15, 20],\n",
    "    'StockPrice': [10.5, 20.3, 30.1, 40.2]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate the target variable (StockPrice)\n",
    "X = df.drop(columns=['StockPrice'])\n",
    "y = df['StockPrice']\n",
    "\n",
    "# Standardize the features (optional but recommended)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)  # Choose the number of components to retain\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bd2057a-fe4b-4cc4-a124-459e5050922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "# values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0895fb39-1e96-45c0-aea1-e0909a559cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X=np.array([1, 5, 10, 15, 20]).reshape(-1, 1)\n",
    "\n",
    "min_max=MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "min_max.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "351b1071-9bff-480d-a1f1-beccfe161ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1],\n",
       "       [ 5],\n",
       "       [10],\n",
       "       [15],\n",
       "       [20]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd30bdda-4dc8-4ddb-9306-a801243c5d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8 For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "# Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0128c20d-dc93-445a-8f13-2b8e5c721cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9607b072-c80f-47b7-ae8d-cf510283f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    [175, 70, 30, 1, 120],\n",
    "    [160, 65, 28, 0, 130],\n",
    "    [180, 80, 35, 1, 140],\n",
    "    [165, 55, 25, 0, 110],\n",
    "    [170, 75, 32, 1, 125]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d91bf6b5-fc0a-4106-9c2b-5efddc836c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=pd.DataFrame(data,columns=['height','weight','age','gender','bp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fbc2bf1-f940-43fb-889e-ccdb26ccae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "data_scaled=scaler.fit_transform(df_new[['height','weight','age','gender','bp']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c54c6908-6477-4243-8a26-3546d594179a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70710678,  0.11624764,  0.        ,  0.81649658, -0.5       ],\n",
       "       [-1.41421356, -0.46499055, -0.58722022, -1.22474487,  0.5       ],\n",
       "       [ 1.41421356,  1.27872403,  1.46805055,  0.81649658,  1.5       ],\n",
       "       [-0.70710678, -1.62746694, -1.46805055, -1.22474487, -1.5       ],\n",
       "       [ 0.        ,  0.69748583,  0.58722022,  0.81649658,  0.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb809cab-023e-4bd0-9d38-b6257390a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9eabcd97-7c16-4565-b26a-05fa6bce539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca = pca.fit_transform(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c963479c-9b83-4bfd-9416-f5c04aea1c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_ratio = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66e5b4f4-b5c6-44fb-8387-c6bdac542e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.10494518e-01, 1.52749455e-01, 3.45256632e-02, 2.23036442e-03,\n",
       "       9.64468529e-35])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_variance_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded96365-e7cd-4da4-9b4b-998fb8f4a85c",
   "metadata": {},
   "source": [
    "we will choose 'height','weight','age','gender'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570605f4-253e-419d-bb3b-b2af73ab0b23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
